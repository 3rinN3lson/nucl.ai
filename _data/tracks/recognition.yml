- title: "Making Robots Interact with People: Lessons Learned Deploying Pepper in Japan"
  description: >
    As one of the main software leads working on Pepper, Gwennael had the chance to deploy an interactive robot in front of real people. In this talk, he’ll describe the key technical things he learnt about AI in the process: how critical it is to back the whole AI stack on a robust perception layer, why building that robust perception layer can only be achieved by merging several sensor inputs (2D vision, 3D vision, audio), how critical but difficult it is to be able to assess performance on such AI-powered systems and more!
  speakers:
    - Gwennael Gaté
  room: amphitheatre
  time:
    start: "14:15"
    finish: "15:00"

- title: Visual Instruments for Live Performance and Creative Expression
  description: >
    Artist and software developer Memo Akten discusses the use of code, computer vision and other algorithms in his work creating interactive installations, music videos, live performances ranging from abstracting olympic gymnasts to a techno-ballet of flying robots. While the work is in a variety of different mediums, they all have in common an underlying theme: exploring ways of using gestural interaction to create and perform visual and sonic art, maintaining a realtime connection to the creation process, allowing finer control over the shaping of the outcome.
  speakers:
    - Memo Akten
  room: amphitheatre
  time:
    start: "12:00"
    finish: "12:45"

- title: Experiments in Augmenting the Performer with AI to Control Live Events
  description: >
    Real-time images, sound synthesizers and effects contain a massive array of parameters. Instead of branching and tuning every parameter between sensors and audiovisuals or using lots of hardware controllers, this is an experiment in using AI techniques to augment the performer's ability to "play" with his setup.
  speakers:
    - Gaspard Bucher
  room: amphitheatre
  time:
    start: "16:30"
    finish: "17:15"

- title: "Advanced Sensors and Tracking Human Interaction in Public Spaces [Workshop]"
  description: >
    In this hands on workshop you will learn how to effeciently track human interaction in public spaces and how to integrate it in your creative applications.  After a brief introduction to openFrameworks, you will learn about the challenges for tracking in public spaces, and get lots of examples of interesting usage scenarios. We will Practice with commercial & industrial sensors data to track people in large sensor areas, as well as day/night indoor/outdoor scenarios. At the end of the workshop you will be able to use the open-source libraries openFrameworks and sensors4Games which will allow you to transform public interaction into a creative app controller.
  speakers:
    - Carles Gutiérrez
  room: laboratories
  time:
    start: "09:15"
    finish: "12:00"

- title: "Diving Into the Next-Generation of AR Devices and Google's Project Tango"
  description: >
    Next-Gen augmented reality (AR) devices are complex systems that unify a multitude of different technologies, ranging from natural user interfaces (NUI) through to AI-assisted input/output. This talk will take a closer look at the past, present and future of augmented reality and dive deep into the architecture of the Google Tango tablet.  Doing so will reveal the advanced AI techniques used to enable AR and provide context for the remaining challenges and problems in this domain.
  speakers:
    - James Bonner
    - Jannes Nagel
  room: masterclass
  time:
    start: "15:00"
    finish: "15:45"

- title: "TBA: Human Recognition & Interface Panel"
  description: >
    *description pending*
  speakers:
    - Richard Kogelnig
  room: masterclass
  time:
    start: "17:15"
    finish: "18:00"
