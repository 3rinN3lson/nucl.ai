- title: "Making Robots Interact with People: Lessons Learned Deploying Pepper in Japan"
  description: >
    As one of the main software leads working on Pepper, Gwennael had the chance to deploy an interactive robot in front of real people. In this talk, he’ll describe the key technical things he learnt about AI in the process: how critical it is to back the whole AI stack on a robust perception layer, why building that robust perception layer can only be achieved by merging several sensor inputs (2D vision, 3D vision, audio), how critical but difficult it is to be able to assess performance on such AI-powered systems and more!
  speakers:
    - Gwennael Gaté
  room: amphitheatre
  time:
    start: "14:15"
    finish: "15:00"

- title: Visual Instruments for Live Performance and Creative Expression
  description: >
    Artist and software developer Memo Akten discusses the use of code, computer vision and other algorithms in his work creating interactive installations, music videos, live performances ranging from abstracting olympic gymnasts to a techno-ballet of flying robots. While the work is in a variety of different mediums, they all have in common an underlying theme: exploring ways of using gestural interaction to create and perform visual and sonic art, maintaining a realtime connection to the creation process, allowing finer control over the shaping of the outcome.
  speakers:
    - Memo Akten
  room: amphitheatre
  time:
    start: "12:00"
    finish: "12:45"

- title: Experiments in Augmenting the Performer with AI to Control Live Events
  description: >
    Real-time images, sound synthesizers and effects contain a massive array of parameters. Instead of branching and tuning every parameter between sensors and audiovisuals or using lots of hardware controllers, this is an experiment in using AI techniques to augment the performer's ability to "play" with his setup.
  speakers:
    - Gaspard Bucher
  room: amphitheatre
  time:
    start: "16:30"
    finish: "17:15"

- title: "TBA: Hands-On OpenFrameworks [Workshop]"
  room: laboratories
  time:
    start: "09:15"
    finish: "11:15"

- title: "AI-powered Next-Gen AR devices: How deep does the rabbit hole go?"
  description: >
    Next-Gen augmented reality (AR) devices are complex systems that unify a multitude of different technologies, ranging from natural user interfaces (NUI) through to AI-assisted input/output. This talk will take a closer look at the past, present and future of augmented reality and dive deep into the architecture of the Google Tango tablet.
Doing so will reveal the advanced AI techniques used to enable AR and provide context for the remaining challenges and problems in this domain.
  speakers:
    - James Bonner
    - Jannes Nagel
  room: masterclass
  time:
    start: "15:00"
    finish: "15:45"

- title: "TBA: Human Interfaces & Recognition Track"
  room: masterclass
  time:
    start: "17:15"
    finish: "18:00"
